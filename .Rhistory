prediction <- list()
for(x in 1:nrow(list_one))
{
PID <- list_one[x,PID]
events_x <- as.character(unlist(list_one[x,Events]))
mcX <- markovchainFit(events_x, method = "map")
pred <- predict(object = mcX$estimate, newdata = events_x, n.ahead=10) # predict next 10 events
prediction[[PID]] <- pred
}
prediction
# Creating final submission file
final_prediction <- data.table(PID = names(prediction), Event = prediction)
for(i in 1:nrow(final_prediction))
{
for(j in 1:10)
{
final_prediction[[paste0("Event",j)]] <- lapply(final_prediction$Event,'[',j)
}
}
final_prediction[,Event := NULL]
fwrite(final_prediction,"markov_map_preds.csv")
View(list_one)
fwrite(list_one,"train_events_all.csv")
?v1
# Load data and libraries
library(data.table)
library(markovchain)
train <- fread('train.csv')
test <- fread('test.csv')
train <- train[order(PID)]
test <- test[order(PID)]
# Create list of events per PID such that event sequence is mainta --------
list_train <- train[,.(list(Event)),.(PID,Date)]
list_one <- list_train[,.(list(V1)),.(PID)]
list_one[,V1 := lapply(V1, unlist, use.names = F)]
setnames(list_one,"V1","Events")
# Building Markov Chain Model on PID Level --------------------------------
prediction <- list()
for(x in 1:nrow(list_one))
{
PID <- list_one[x,PID]
events_x <- as.character(unlist(list_one[x,Events]))
mcX <- markovchainFit(events_x, method = "laplace", laplacian = 0.01)
pred <- predict(object = mcX$estimate, newdata = events_x, n.ahead=10) # predict next 10 events
prediction[[PID]] <- pred
}
prediction
# Creating final submission file
final_prediction <- data.table(PID = names(prediction), Event = prediction)
for(i in 1:nrow(final_prediction))
{
for(j in 1:10)
{
final_prediction[[paste0("Event",j)]] <- lapply(final_prediction$Event,'[',j)
}
}
final_prediction[,Event := NULL]
fwrite(final_prediction,"markov_laplace_preds.csv")
install.packages(c('tm', 'SnowballC', 'wordcloud', 'topicmodels'))
devtools::install_github("bmschmidt/wordVectors")
install.packages('text2vec')
setwd('C:\Users/MAYANK/Desktop/ZS INTERVIEW/')
library(text2vec)
library(data.table)
data("movie_review")
setDT(movie_review)
setkey(movie_review, id)
set.seed(2016L)
all_ids = movie_review$id
train_ids = sample(all_ids, 4000)
test_ids = setdiff(all_ids, train_ids)
train = movie_review[J(train_ids)]
test = movie_review[J(test_ids)]
library(text2vec)
library(data.table)
data("movie_review")
View(movie_review)
library(tm)
library(SnowballC)
library(wordcloud)
library(data.table)
review = data("movie_review")
review <- data("movie_review")
review <- review[, c("content", "polarity")]
# Importing the dataset and preparing of dataset
review <- data("movie_review")
review <- review[, c("content", "polarity")]
review <- movie_review[, c("content", "polarity")]
# Importing the dataset and preparing of dataset
data("movie_review")
review <- movie_review[, c("content", "polarity")]
View(movie_review)
review[,c("content", "polarity")] <- movie_review[,c("review", "sentiment") ]
review[,c("content", "polarity")] <- movie_review[,c(3,2) ]
movie_review[, "sentiment"]
movie_review[,c("sentiment", "review")]
movie_review[,c("review","sentiment")]
review <- movie_review[,c("review","sentiment")]
View(review)
review.colnames
colnames(review)
colnames(review) <- c("content", "polarity")
review_corpus = Corpus(VectorSource(review))
review_corpus = tm_map(review_corpus, content_transformer(tolower))
review_corpus = tm_map(review_corpus, removeNumbers)
review_corpus = tm_map(review_corpus, removePunctuation)
review_corpus = tm_map(review_corpus, removeWords, c("the", "and", stopwords("english")))
review_corpus = tm_map(review_corpus, stripWhitespace)
inspect(review_corpus[1])
review_dtm <- DocumentTermMatrix(review_corpus)
review_dtm
inspect(review_dtm[500:505, 500:505])
inspect(review_dtm[500:505, 50:55])
inspect(review_dtm[50:55, 50:55])
inspect(review_dtm[50:55,])
inspect(review_dtm)
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
freq = data.frame(sort(colSums(as.matrix(review_dtm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
review_dtm_tfidf <- DocumentTermMatrix(review_corpus, control = list(weighting = weightTfIdf))
review_dtm_tfidf = removeSparseTerms(review_dtm_tfidf, 0.95)
review_dtm_tfidf
review_dtm_tfidf <- DocumentTermMatrix(review_corpus, control = list(weighting = weightTfIdf))
#review_dtm_tfidf = removeSparseTerms(review_dtm_tfidf, 0.95)
review_dtm_tfidf
freq = data.frame(sort(colSums(as.matrix(review_dtm_tfidf)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=100, colors=brewer.pal(1, "Dark2"))
install.packages("glmnet", repos = "http://cran.us.r-project.org")
library(text2vec)
library(data.table)
# Importing the data
data("movie_review")
movie_review
setDT(movie_review)
setkey(movie_review, id)
set.seed(2016L)
all_ids = movie_review$id
train_ids = sample(all_ids, 4000)
test_ids = setdiff(all_ids, train_ids)
train = movie_review[J(train_ids)]
test = movie_review[J(test_ids)]
# define preprocessing function and tokenization fucntion
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(train$review,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = train$id,
progressbar = FALSE)
vocab = create_vocabulary(it_train)
vocab
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_train)
identical(rownames(dtm_train), train$id)
library(glmnet)
NFOLDS = 4
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']],
family = 'binomial',
# L1 penalty
alpha = 1,
# interested in the area under ROC curve
type.measure = "auc",
# 5-fold cross-validation
nfolds = NFOLDS,
# high value is less accurate, but has faster training
thresh = 1e-3,
# again lower number of iterations for faster training
maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))
plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
# Note that most text2vec functions are pipe friendly!
it_test = test$review %>%
prep_fun %>%
tok_fun %>%
itoken(ids = test$id,
# turn off progressbar because it won't look nice in rmd
progressbar = FALSE)
dtm_test = create_dtm(it_test, vectorizer)
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)
stop_words = c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours")
t1 = Sys.time()
vocab = create_vocabulary(it_train, stopwords = stop_words)
print(difftime(Sys.time(), t1, units = 'sec'))
pruned_vocab = prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.5,
doc_proportion_min = 0.001)
vectorizer = vocab_vectorizer(pruned_vocab)
# create dtm_train with new pruned vocabulary vectorizer
t1 = Sys.time()
dtm_train  = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_train)
dtm_test   = create_dtm(it_test, vectorizer)
dim(dtm_test)
t1 = Sys.time()
vocab = create_vocabulary(it_train, ngram = c(1L, 2L))
print(difftime(Sys.time(), t1, units = 'sec'))
vocab = vocab %>% prune_vocabulary(term_count_min = 10,
doc_proportion_max = 0.5)
bigram_vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, bigram_vectorizer)
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']],
family = 'binomial',
alpha = 1,
type.measure = "auc",
nfolds = NFOLDS,
thresh = 1e-3,
maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))
plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
# apply vectorizer
dtm_test = create_dtm(it_test, bigram_vectorizer)
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)```
x <- runif(1000, -5, 5)
y <- x + rnorm(1000) + 3
res <- lm(y ~ x)
print(res)
plot(x, y, col= rgb(0.2, 0.4, 0.6, 0.4), main = 'Linear Regression by gradient descent')
abline(res, col='blue')
cost <- function(X, y, theta) {
sum ((X %*% theta - y)^ 2) / (2 * length(y))
}
alpha <- 0.01
num_iters <- 1000
cost_history <- double(num_iters)
theta_history <- list(num_iters)
theta <- matrix(c(0, 0), nrow = 2)
View(theta)
View(theta)
X <- cbind(1, matrix(X))
X <- cbind(1, matrix(x))
for ( i in 1:num_iters){
error <- (X %*% theta - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost(X, y, theta)
theta_history[i] <- theta
}
print(theta)
# plot data and converging fit
plot(x,y, col=rgb(0.2,0.4,0.6,0.4), main='Linear regression by gradient descent')
for (i in c(1,3,6,10,14,seq(20,num_iters,by=10))) {
abline(coef=theta_history[[i]], col=rgb(0.8,0,0,0.3))
}
# plot data and converging fit
plot(x,y, col=rgb(0.2,0.4,0.6,0.4), main='Linear regression by gradient descent')
abline(coef=theta, col='blue')
# plot data and converging fit
plot(x,y, col=rgb(0.2,0.4,0.6,0.4), main='Linear regression by gradient descent')
abline(coef=theta, col='blue')
for (i in c(1,3,6,10,14,seq(20,num_iters,by=10))) {
abline(coef=cost_history[[i]], col=rgb(0.8,0,0,0.3))
}
# plot data and converging fit
plot(x,y, col=rgb(0.2,0.4,0.6,0.4), main='Linear regression by gradient descent')
abline(coef=theta, col='blue')
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
x <- runif(1000, -5, 5)
y <- x + rnorm(1000) + 3
res <- lm(y ~ x)
print(res)
plot(x, y, col= rgb(0.2, 0.4, 0.6, 0.4), main = 'Linear Regression by gradient descent')
abline(res, col='blue')
cost <- function(X, y, theta) {
sum ((X %*% theta - y)^ 2) / (2 * length(y))
}
alpha <- 0.01
num_iters <- 1000
cost_history <- double(num_iters)
theta_history <- list(num_iters)
theta <- matrix(c(0, 0), nrow = 2)
X <- cbind(1, matrix(x))
for ( i in 1:num_iters){
error <- (X %*% theta - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost(X, y, theta)
theta_history[i] <- theta
}
print(theta)
# plot data and converging fit
plot(x,y, col=rgb(0.2,0.4,0.6,0.4), main='Linear regression by gradient descent')
abline(coef=theta, col='blue')
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
## load data
loadData <- function() {
missingTypes <- c(NA, '', ' ')
trips <<- read.csv('./data/hubway_trips.csv', na.strings=missingTypes, stringsAsFactors=FALSE)
stations <<- read.csv('./data/hubway_stations.csv', na.strings=missingTypes, stringsAsFactors=FALSE)
}
usePackage <- function(p) {
if (!is.element(p, installed.packages()[,1]))
install.packages(p, dep = TRUE)
require(p, character.only = TRUE)
}
loadLibraries <- function() {
usePackage('dplyr')
usePackage('reshape2')
}
#### casting data types
casteDataTypeForTripsDF <- function(tripsDF) {
tripsDF$seq_id <- NULL
tripsDF$status <- NULL
tripsDF$start_date <- strptime(tripsDF$start_date, format='%m/%d/%Y %H:%M:%S')
tripsDF$end_date <- strptime(tripsDF$end_date, format='%m/%d/%Y %H:%M:%S')
tripsDF$bike_nr <- as.factor(tripsDF$bike_nr)
tripsDF$subsc_type <- as.factor(tripsDF$subsc_type)
tripsDF$zip_code <- NULL
tripsDF$birth_date <- as.factor(tripsDF$birth_date)
tripsDF$gender <- as.factor(tripsDF$gender)
return(tripsDF)
}
casteDataTypeForStations <- function(stationsDF) {
stationsDF$terminal <- as.factor(stationsDF$terminal)
stationsDF$station <- as.factor(stationsDF$station)
stationsDF$municipal <- as.factor(stationsDF$municipal)
stationsDF$status <- as.factor(stationsDF$status)
return(stationsDF)
}
removeRowsMissStns <- function(tripsDF) {
tripsDF <- subset(tripsDF, !is.na(strt_statn) & !is.na(end_statn))
return(tripsDF)
}
## aggregate trips from station to station
aggTripsS2S <- function(tripsDF) {
tripsDF$start_date <- as.character(tripsDF$start_date)
tripsDF$end_date <- as.character(tripsDF$end_date)
grp <- group_by(tripsDF, strt_statn, end_statn)  # set up the grouping
agg <- dplyr::summarize(grp, cnt=n())  #set up aggregation by groups
agg <- arrange(agg, cnt)  # order the data
agg <- collect(agg)  # grab the result
agg <- as.data.frame(agg)
return(agg)
}
## add longitude and latitude info for starting stations
addStartLocs <- function(df) {
df$strtLng <- stations$lng[match(df$strt_statn, stations$id)]
df$strtLat <- stations$lat[match(df$strt_statn, stations$id)]
return(df)
}
## add longitude and latitude info for ending stations
addEndLocs <- function(df) {
df$endLng <- stations$lng[match(df$end_statn, stations$id)]
df$endLat <- stations$lat[match(df$end_statn, stations$id)]
return(df)
}
## add longitude and latitude information to start and end stations
addStartAndEndLocs <- function(trAggDF) {
trAggDF <- addStartLocs(trAggDF)
trAggDF <- addEndLocs(trAggDF)
return(trAggDF)
}
## dplyr count method
dplyrCnt <- function(grp) {
agg <- dplyr::summarize(grp, cnt=sum(cnt))  # aggregate
agg <- arrange(agg, cnt)  # order
agg <- collect(agg)
agg <- as.data.frame(agg)
return(agg)
}
## standardize column names for trip-counts-by-station dfs
stdTripCtnByStnDFColnames <- function(df) {
colnames(df) <- tolower(colnames(df))
colnames(df) <- gsub('^end(_)?|^strt(_)?|^start(_)?', '', colnames(df))
return(df)
}
## add station locations
addStnLocs <- function(df) {
df$lng <- stations$lng[match(df$statn, stations$id)]
df$lat <- stations$lat[match(df$statn, stations$id)]
return(df)
}
## aggregate number of incoming/outgoing trips by station id
aggTripCntByStn <- function(trAggDF) {
## incoming trip counts by station
grp <- group_by(trAggDF, end_statn)
incTripCntByStnDF <- dplyrCnt(grp)
incTripCntByStnDF <- stdTripCtnByStnDFColnames(incTripCntByStnDF)
colnames(incTripCntByStnDF)[2] <- 'inc_cnt'
## outgoing trip counts by station
grp <- group_by(trAggDF, strt_statn)
outTripCntByStnDF <- dplyrCnt(grp)
outTripCntByStnDF <- stdTripCtnByStnDFColnames(outTripCntByStnDF)
colnames(outTripCntByStnDF)[2] <- 'out_cnt'
## combine the two dfs, order, and return
outputDF <- merge(incTripCntByStnDF, outTripCntByStnDF, by='statn')
outputDF$tot_cnt <- outputDF$inc_cnt + outputDF$out_cnt
outputDF <- outputDF[order(outputDF$statn), ]
return(outputDF)
}
## add incoming and outgoing trip percentages
addIncOutTripPercs <- function(byStnMetricDF) {
byStnMetricDF$inc_perc <- round(byStnMetricDF$inc_cnt / byStnMetricDF$tot_cnt * 100, 2)
byStnMetricDF$out_perc <- round(byStnMetricDF$out_cnt / byStnMetricDF$tot_cnt * 100, 2)
return(byStnMetricDF)
}
## function that takes in station-to-station pairs and collapses directionality of trips
## e.g. stnID4-stnID120 trip and stnID120-stnID4 trip should be combined into one pair (either as stnID4-stnID120 or stnID120-stnID4)
collapseDirectionS2S <- function(trAggDF) {
output <- trAggDF
## collapse directionality dimension
output$strtLng <- output$strtLat <- output$endLng <- output$endLat <- NULL
output[1:2] <- t(apply(output, 1, function(x) sort(x[1:2])))
output <- aggregate(cnt ~ ., data=output, FUN=sum)
## order the output df by strt_statn and end_statn
output <- output[order(output$strt_statn, output$end_statn), ]
## add start and end locations
output <- addStartAndEndLocs(output)
return(output)
}
loadData()
loadLibraries()
## aggregating
trAggS2S <- aggTripsS2S(trips)
trAggS2S <- addStartAndEndLocs(trAggS2S)
ndTrAggS2S <- collapseDirectionS2S(trAggS2S)
byStnMetric <- aggTripCntByStn(trAggS2S)
byStnMetric <- addIncOutTripPercs(byStnMetric)
byStnMetric <- addStnLocs(byStnMetric)
View(trips)
View(ndTrAggS2S)
View(trAggS2S)
tableau_path <- data.frame(statn = numeric(),
path_id = character(),
cnt = numeric(),
lat = numeric(),
long = numeric())
View(tableau_path)
for (row in c(1:10)) {
df_new <- data.frame(trips[row, strt_statn])
print(df_new)
}
for (row in c(1:10)) {
df_new <- data.frame(trAggS2S[row, strt_statn])
print(df_new)
}
for (row in c(1:10)) {
df_new <- data.frame(trAggS2S[row, "strt_statn"])
print(df_new)
}
View(df_new)
for (row in c(1:10)) {
df_new <- data.frame(trAggS2S[row, "strt_statn"])
view(df_new)
}
for (row in c(1:10)) {
df_new <- data.frame(trAggS2S[row, "strt_statn"])
}
for (row in c(1:10)) {
df_new <- data.frame(trAggS2S[row, "strt_statn"], trAggS2S[row, "strtLng"], trAggS2S[row,"strtLat"])
}
View(df_new)
for (row in c(1:10)) {
df_new <- data.frame(trAggS2S[row, "strt_statn"],as.character(trAggS2S[row, "strt_statn"]) + "_" + as.character(trAggS2S[row, "end_statn"]),trAggS2S[row, "cnt"], trAggS2S[row, "strtLng"], trAggS2S[row,"strtLat"])
}
for (row in c(1:10)) {
df_new <- data.frame(trAggS2S[row, "strt_statn"],paste(as.character(trAggS2S[row, "strt_statn"]),as.character(trAggS2S[row, "end_statn"]), collapse = '_'),trAggS2S[row, "cnt"], trAggS2S[row, "strtLng"], trAggS2S[row,"strtLat"])
}
for (row in c(1:10)) {
df_new <- data.frame(trAggS2S[row, "strt_statn"],paste(as.character(trAggS2S[row, "strt_statn"]),as.character(trAggS2S[row, "end_statn"]), sep = '_'),trAggS2S[row, "cnt"], trAggS2S[row, "strtLng"], trAggS2S[row,"strtLat"])
}
for (row in c(1:nrow(trAggS2S))) {
df_new_start <- data.frame(trAggS2S[row, "strt_statn"],paste(as.character(trAggS2S[row, "strt_statn"]),as.character(trAggS2S[row, "end_statn"]), sep = '_'),trAggS2S[row, "cnt"], trAggS2S[row, "strtLng"], trAggS2S[row,"strtLat"])
tableau_path <- rbind(tableau_path, df_new_start)
df_new_end <- data.frame(trAggS2S[row, "end_statn"],paste(as.character(trAggS2S[row, "strt_statn"]),as.character(trAggS2S[row, "end_statn"]), sep = '_'),trAggS2S[row, "cnt"], trAggS2S[row, "endLng"], trAggS2S[row,"endLat"])
tableau_path <- rbind(tableau_path, df_new_end)
}
tableau_path <- data.frame(statn = numeric(),
path_id = character(),
cnt = numeric(),
lat = numeric(),
long = numeric())
for (row in c(1:nrow(trAggS2S))) {
df_new_start <- data.frame(trAggS2S[row, "strt_statn"],paste(as.character(trAggS2S[row, "strt_statn"]),as.character(trAggS2S[row, "end_statn"]), sep = '_'),trAggS2S[row, "cnt"], trAggS2S[row, "strtLng"], trAggS2S[row,"strtLat"])
names(df_new_start) <- names(tableau_path)
tableau_path <- rbind(tableau_path, df_new_start)
df_new_end <- data.frame(trAggS2S[row, "end_statn"],paste(as.character(trAggS2S[row, "strt_statn"]),as.character(trAggS2S[row, "end_statn"]), sep = '_'),trAggS2S[row, "cnt"], trAggS2S[row, "endLng"], trAggS2S[row,"endLat"])
names(df_new_end) <- rbind(tableau_path)
tableau_path <- rbind(tableau_path, df_new_end)
}
tableau_path <- data.frame(statn = numeric(),
path_id = character(),
cnt = numeric(),
lat = numeric(),
long = numeric())
for (row in c(1:nrow(trAggS2S))) {
df_new_start <- data.frame(trAggS2S[row, "strt_statn"],paste(as.character(trAggS2S[row, "strt_statn"]),as.character(trAggS2S[row, "end_statn"]), sep = '_'),trAggS2S[row, "cnt"], trAggS2S[row, "strtLng"], trAggS2S[row,"strtLat"])
names(df_new_start) <- c("statn", "path_id", "cnt", "lat", "long")
tableau_path <- rbind(tableau_path, df_new_start)
df_new_end <- data.frame(trAggS2S[row, "end_statn"],paste(as.character(trAggS2S[row, "strt_statn"]),as.character(trAggS2S[row, "end_statn"]), sep = '_'),trAggS2S[row, "cnt"], trAggS2S[row, "endLng"], trAggS2S[row,"endLat"])
names(df_new_end) <- c("statn", "path_id", "cnt", "lat", "long")
tableau_path <- rbind(tableau_path, df_new_end)
}
getwd()
setwd("C:/Users/MAYANK/Desktop/Blog/hubway-visualization-challenge/data/")
getwd()
cd ..
setwd("C:/Users/MAYANK/Desktop/Blog/hubway-visualization-challenge")
getwd()
write.csv(tableau_path, file = './data/tableau_path.csv')
